# -*- coding: utf-8 -*-
"""actor_critic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Gpya9yswf-xonOvKhoHpmQGhCYpq8x4
"""

# !pip install box2d-py
# !pip install gym[Box_2D]

import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
import gym
import os

class ActorCritic(nn.Module):
    def __init__(self, num_actions):
        super().__init__()
        self.fc2 = nn.Linear(8, 2048)
        self.fc3 = nn.Linear(2048, 512)
        self.pi = nn.Linear(512, num_actions)
        self.v = nn.Linear(512, 1)
        
    def forward(self, x):
#         x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        pi = self.pi(x)
        v = self.v(x)
        return pi, v

class Agent():
    def __init__(self):
        self.gamma = 0.99
        self.log_probs = None
        self.env = gym.make('LunarLander-v2')
        num_actions = self.env.action_space.n
        self.ac_network = ActorCritic(num_actions=num_actions).cuda()
        self.MODEL_PATH = 'ac_model.pth'
        if os.path.exists(self.MODEL_PATH):
            print('Existing model found!')
            self.ac_network.load_state_dict(torch.load(self.MODEL_PATH))
            self.ac_network.eval()
        else:
            print('No existing model.')
        self.optimizer = torch.optim.Adam(self.ac_network.parameters(), lr=1e-5)
        self.MAX_FRAMES = 10000
        self.NUM_EPISODES = 3000
        
    def choose_action(self, state):
        policy, _ = self.ac_network(torch.tensor(state).cuda())
        probablities = F.softmax(policy)
        action_distribution = torch.distributions.Categorical(probablities)
        action = action_distribution.sample()
        self.log_probs = action_distribution.log_prob(action)
        return action.item()
    
    def learn(self, state, next_state, reward, done):
#         with torch.no_grad():
        self.optimizer.zero_grad()
        _, curr_value = self.ac_network(torch.tensor(state).cuda())
        _, next_value = self.ac_network(torch.tensor(next_state).cuda())
        reward = torch.tensor(reward).cuda()
        
        if done:
            advantage = reward - curr_value
        else:
            advantage = reward + self.gamma * next_value - curr_value
            
        actor_loss = -self.log_probs * advantage
        critic_loss = advantage ** 2
        
        loss = actor_loss + critic_loss
#         loss.item()
        loss.backward()
        self.optimizer.step()
        return loss.item()
        
    def play(self):
        for i_episode in range(self.NUM_EPISODES):
            observation = self.env.reset()
            curr_state = observation
            done = False
            total_rewards = 0
            total_loss = 0
            num_frames = 0
            while not done:
                self.env.render()
                action = self.choose_action(curr_state)
                observation, reward, done, info = self.env.step(action)
                # loss = self.learn(state=curr_state, next_state=observation, reward=reward, done=done)
                curr_state = observation
                total_rewards += reward
                # total_loss += loss
                num_frames += 1
                
            print(f'Episode # {i_episode} done, total reward: {total_rewards}')

        self.env.close()

agent = Agent()
agent.play()
torch.save(agent.ac_network.state_dict(), agent.MODEL_PATH)